:PROPERTIES:
:TOC:      :include all :depth 2
:END:
#+TITLE: Engenharia de Dados
:CONTENTS:
1. [[#abertura---como-trabalhar-em-um-ambiente-%C3%A1gil-e-global][Abertura - Como trabalhar em um ambiente ágil e global]]

1. [[#monitoramento-de-clusters-hadoop-de-alto-n%C3%ADvel-com-hdfs-e-yarn][Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn]]

1. [[#orquestrando-ambientes-de-big-data-distruibuidos-com-zookeeper-yarn-e-sqoop][Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop]]

1. [[#como-realizar-consultas-de-maneira-simples-no-ambiente-complexo-de-big-data-com-hive-e-impala][Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala]]

1. [[#trabalhando-com-servi%C3%A7os-de-mensageria-real-time-com-python-e-kafka-na-pr%C3%A1tica][Trabalhando com serviços de mensageria real time com Python e Kafka na prática]]
:END:

* Abertura - Como trabalhar em um ambiente ágil e global
Kick-off da Aceleração.  A live começou com a apresentação da Everis, empresa de
consultoria multinacional que estará ministrando as palestras técnicas sobre o
assunto de Big Data. Entraram em detalhes sobre a filosofia e cultura da
empresa, bem como ressaltaram que há oportunidades na área de Dados e que
poderão ser oferecidas aos alunos de destaque do curso ao fim do treinamento.

Esse curso contou com um total de 2000 candidaturas. Por volta de 400 foram
selecionados após realizar uma prova eliminatória. A prova contou com questões
de multipla escolha sobre Python e lógica de programação, além de um coding
test.

Serão 14 dias de treinamento passando pelas linguagens de programação Python e
Scala além de ferramentas fundamentais para Big Data. Serão dadas noções do
_core_ Hadoop com HDFS, Yarn, além de outras ferramentas do ecossistema como
MapReduce, Sqoop, Hive, Zookeeper e Spark.  Também serão cobertas noções de
bancos de dados NoSQL como Cassandra e HBase.

Foi ressaltada a importância da mudança do paradigma Waterfall para o paradigma
Agile em projetos de Engenharia de Dados.  Planejamento e execução de Sprints,
Dailies, Reviews, organização em Squads autosuficientes, constante interação com
o cliente, foco na entrega. Essas são algumas das características essenciais da
cultura Agile e que contribuem para o sucesso de projetos utilizando esse
framework.

#+html: <p align="center"> <figure>
#+html: <img src="figuras/BIG_AgileProcess-V2.png" />
#+html: <figcaption>Framwork Agile. Créditos: <a href="https://bitsinglass.com/agile-methodologies-enhance-appian-delivery-part-1/">bitsinglass</a> </figcaption>
#+html: </figure> </p>

Agile requer um processo de comunicação forte, com transparência, confiânça e
união entre os colaboradores das Squads. No geral, focaram no fato de que
colaboração é um aspecto fundamental do framework Agile.

* Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn
** Sistemas distribuídos
Clusters de computadores hoje em dia são uma alternativa aos antigos mainframes.
Enquanto mainframes conseguem oferecer escalabilidade vertical, cluters
conseguem oferecer escalabilidade horizontal. Isso é, para conseguir mais
recursos de armazenamento e computação para os mainframes você precisa

Nesse contexto, demos início ao estudo do Hadoop, uma tecnologia desenvolvida
utilizando princípios de sistemas distribuídos e que, portanto, se encaixam
nessa nova realidade dos clusters. Passamos por uma descrição alto nível do que
acontece por debaixo dos panos quando nós, desenvolvedores, interagimos com o
HDFS e Yarn.

** Hadoop
Essas duas tecnologias são as peças fundamentais que sustentam o ecossistema
Hadoop. Os nomes HDFS e Yarn podem parecer uma sopa de letrinhas sem muito
sentido para quem os vê fora de contexto. Mas note que ambos na verdade são
siglas bastante descritivas do que essas tecnologias se propõem a fazer.

O HDFS, ou Hadoop Distributed File System, fica encarregado de distribuir e
gerenciar os seus arquivos entre as máquinas do seu cluster. Já o Yarn, Yet
Another Resource Negotiator, é responsável por negociar recrusos computacionais
para as suas tarefas de manipulação de dados.

#+html: <figure align="center">
#+html: <img src="figuras/hadoop_eco.png" />
#+html: <figcaption>Ecossistema Hadoop. Créditos: <a href="https://dzone.com/articles/example-of-etl-application-using-apache-spark-and">dzone</a> </figcaption>
#+html: </figure>

* Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop
** Zookeepr
Serviço de coordenação distribuído.
Fornece as rotas necessárias para as peças do cluster. Identifica nós por nomes (DSN-like).
Ajuda as peças do ecossitemas Hadoop se achar na bagunça toda.
Pode ajudar a evitar concorrência (write on read).
Ajuda na recuperação de falhas.

Para o desenvolvedor, o Zookeeper é completamente transparente.
É mais aparente para quem vai trabalhar com infra.

** Sqoop
Movimenta dados entre banco de dados relacional e HDFS.  Realiza a leitura linha
a linha de tabelas para arquivos (pode pecar na performance).  Permite importar
dados e metadados de bancos de dado relacionais direto para o Hive.  Utiliza
MapReduce por debaixo dos panos: processamento paralelo e tolerante a falha.
Contudo MapReduce já não é mais o motor mais rápido disponível (alternativas:
Spark, Flink).

#+BEGIN_SRC bash
sqoop import \
    --connect jdbc: \
    --username abc \
    --password xyz \
    --table some_table \
    --where "column='something'"
#+END_SRC

*** Desafio
**** SQL
1. Todos os Pokémon lendários
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE legendary=TRUE;
   #+END_SRC
2. Todos os Pokémon de apenas um tipo
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE type2='';
   #+END_SRC
3. Os top 10 Pokémon mais rápidos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY speed DESC
    LIMIT 10;
   #+END_SRC
4. Os top 50 Pokémon com menos HP
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY hp ASC
    LIMIT 50;
   #+END_SRC
5. Os top 100 Pokémon com maiores atributos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY
    greatest(hp, attack, defense, spatk, spdef, speed)
    DESC
    LIMIT 100;
   #+END_SRC

**** Sqoop

** Recursos
http://dontpad.com/aceleracaoeveris
https://drive.google.com/drive/folders/1xaft6H3R3_UvA6-BFHuCvHuWczf6xwqG?usp=sharing

* Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala
Hive e Impala são frontends que possibilitam um a conexão facilitada aos dados no HDFS.

** Hive
Hive oferece a linguagem HQL (Hive Query Langue), uma abstração de alto nível ao MapReduce, com linguagem similar ao conhecido SQL.

Três possíveis engines MapReduce, Spark, Tez.
** Impala
Engine MPP (Massive Parallel Processing). Também uma linguagem similar a SQL (Impala SQL).

Impala não salva os resultados intermediários em disco. Isso acelera e muito o
processamento em comparação com o Hive.

** Diferenças
Casos de uso:
Realtime -> Impala
Batch -> Hive

Impala não salva intermediários em disco, fica tudo em memória. Portanto seu
consumo de memória é muito maior, o que pode ser um limitante dependendo do tipo
de queries executadas.

** Detalhes HQL
CREATE EXTERNAL vs MANAGED TABLE
External - Quando a tablea é apagada, os dados permanecem
Managed - Apaga os dados quando a tabela é deletada

** Formatos de arquivos
*** Parquet
Formato Colunar
*** ORC
Formato Colunar
*** Avro
Formato de Linhas

** Particionamento
Determina como os dados são armazenados.

Pouco particionamento: não faz bom uso da capacidade de paralelismo dos dados
Muito particionamento: pode sobrecarregar o namenode, impactando na performance

** Prática
*** Hive
Utilitários
#+BEGIN_SRC sql
set hive.cli.print.header=true;
set hive.cli.print.current.db=true;
#+END_SRC

Criar tabela em cima de pasta do HDFS.
#+BEGIN_SRC sql
CREATE EXTERNAL TABLE TB_EXT_EMPLOYEE(
id STRING,
groups STRING,
age STRING,
active_lifestyle STRING,
salary STRING)
ROW FORMAT DELIMITED FIELDS
TERMINATED BY '\;'
STORED AS TEXTFILE
LOCATION '/user/hive/warehouse/external/tabelas/employee'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Enviar dados para a LOCATION especificada pela tabela acima.
#+BEGIN_SRC sql
hdfs dfs -put /home/everis/employee.txt /user/hive/warehouse/external/tabelas/employee
#+END_SRC

Melhorar tabela acima com os tipos apropriados.
#+BEGIN_SRC sql
CREATE TABLE TB_EMPLOYEE(
id INT,
groups STRING,
age INT,
active_lifestyle STRING,
salary DOUBLE)
PARTITIONED BY (dt_processamento STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS PARQUET TBLPROPERTIES ("parquet.compression"="SNAPPY");

insert into table TB_EMPLOYEE partition (dt_processamento='20201118') 
select
id,
groups,
age,
active_lifestyle,
salary
from TB_EXT_EMPLOYEE;
#+END_SRC

Criar tabela em cima de pasta no HDFS (segundo exemplo)
#+BEGIN_SRC sql
create external table localidade(
street string,
city string,
zip string,
state string,
beds string,
baths string,
sq_ft string,
type string,
sale_date string,
price string,
latitude string,
longitude string)
PARTITIONED BY (particao STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ","
STORED AS TEXTFILE
location '/user/hive/warehouse/external/tabelas/localidade'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Alternativamente, criar tabela com base em arquivo.
Hive envia dados para o HDFS automaticamente.
#+BEGIN_SRC sql
load data local inpath '/home/everis/base_localidade.csv' 
into table teste.localidade partition (particao='2021-01-21');
#+END_SRC

**** Join
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins

** Dia-a-dia
/home/cloudera/hive/script.sh
#+BEGIN_SRC shell
#!/bin/bash

dt_processamento=$(date '+%Y-%m-%d')
path_file='/home/cloudera/hive/datasets/employee.txt'
table=beca.ext_p_employee
load=/home/cloudera/hive/load.hql

hive -hiveconf dt_processamento=${dt_processamento} -hiveconf table=${table} -hiveconf path_file=${path_file} -f $load 2>> log.txt

hive_status=$?

if [ ${hive_status} -eq 0 ];
then
        echo -e "\nScript executado com sucesso"
else
        echo -e "\nHouve um erro na ingestao do arquivo "

impala-shell -q 'INVALIDATE METADATA beca.ext_p_employee;'

fi
#+END_SRC

/home/cloudera/hive/load.hql
#+BEGIN_SRC shell
LOAD DATA LOCAL INPATH '${hiveconf:path_file}' INTO TABLE ${hiveconf:table} PARTITION(dt_processamento='${hiveconf:dt_processamento}');
#+END_SRC

** Recursos
https://gitlab.com/vmb1/hive

* Explorando o poder do NoSQL com Cassandra e Hbase
Por quê NoSQL em Big Data? Resposta: Performance.

MapReduce processa dados em batch e os dados são acessados de forma sequencial. Ou seja, é preciso percorrer todo o dataset (scan search), mesmo para jobs mais simples.

NoSQL possibilita acesso aleatório aos dados em termos de posição do registro e tempo.

** CAP
Teorema CAP: Consistência, Disponibilidade, Partição Tolerante a Falhas.

** HBase
Banco de dados distribuído e orientado a coluna (Column Family ou Wide Column).

É um Map:
- Esparso
- Distribuído
- Persistente
- Multidimensional
- Ordenado

Depende do Zookeeper para que consiga funcionar. O Zookeeper dá ao HBase visibilidade a todos os nós do cluster.

**** Desvantagem
- Infelizmente não possui uma linguagem de busca (query)
- Não suporta índices em colunas fora da rowkey
- Não suporta tabelas secundárias de índices

**** Vantagem
- Fácil integração ao ecossistema Hadoop

*** Estrutura
- Map indexado por uma linha chave (row key), coluna chave (column key) e uma coluna timestamp.
- Cada valor no Map é interpretado como um vetor de bytes (array of bytes)
- Não distingue tipos (int, str, etc): pode armazenar qualquer tipo de dado, inclusive documentos (JSON, CSV, ...)

{

}

*** Arquitetura
*** Exemplos
Criação de tabela

Inserção de dados

Versionamento

Deletar

Deletar em coluna versionada

TTL
create 'ttl_exemplo', {'NAME'=>'cf', 'TTL'=>20}
put 'ttl_exemplo', '1', 'cf:nome', 'Informacao'
scan 'ttl_exemplo'
** Cassandra
Banco de dados distribuído e orientado a coluna (Column Family ou Wide Column).

Os dados aqui são tipados.

Possui linguagem CQL (SQL-like), porém com algumas operações não suportadas/recomendadas (eg joins, alguns tipos de agrupamento e filto).

Suporta tabela secundárias de índices e filtros em colunas fora da primary key.

*** Arquitetura
Não possui ponto de falha central.

Conexão entre nós é realizada de ponta a ponta, utilizando o protocolo Gossip para distribuição dos dados.

Commit table -> memtable -> SSTable

** TTL
Registro temporário: colunas com propriedade TTL - Time To Live. Os registros são apagados depois desse período.

** Cenários de uso
- Arquitetura baseada em eventos
    Kafka Cluster -> Spark Streaming -> HBase/Cassandra -> Enriquecimento -> Destino

** Recursos
https://github.com/pentguard/DIO-Aceleracao-4-HBase-Cassandra
https://www.datastax.com/

** Etc
sudo -u hdfs hadoop dfsadmin -safemode leave

Por que o safemode é ativado? Arquivos corrompidos.

É importante parar todos os serviços do Hadoop antes de desligar a máquina.

Como resolver: recuperar os arquivos (caso haja replicação) ou apagar.

* Intensivo de Python: O mínimo que você precisa saber
* Trabalhando com serviços de mensageria real time com Python e Kafka na prática
** Introdução a Micro Serviços
Existem diferentes tipos de arquiteturas.
*** Monolito
Banco + Aplicação (front + back) tudo no mesmo servidor.
**** Desvantagens
**** Vantagens
Comunicação simplificada dentro do sistema
*** Micro Serviços
O sistema é dividido em diversos "módulos" (diversas partes).
**** Desvantagens
- Complexidade em coordenar a comunicação de diferentes servidores/serviços
- Comunicação pode gerar acoplamento entre os módulos
  - Pode ser resolvido através de um serviço de mensageria!
**** Vantagens
- Isolamento de responsabilidades
  Melhor manutenibilidade/sustentação
- Controle granulirizado da escala de cada um dos serviços
  Escalabilidade facilitada

** Kafka
Sistema de mensageria Open Source.

> Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.

#+html: <p align="center"> <figure>
#+html: <img src="figuras/kafka_ex.png" />
#+html: <figcaption>Visão geral dos componentes Kafka. Créditos: <a href="https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a">bitsinglass</a> </figcaption>
#+html: </figure> </p>

*** Producers
Geram mensagens, que são enviadas para uma fila (mais especificamente tópicos, no Kafka), a serem lidas por determinados Consumers

#+BEGIN_SRC python
topic = 'topic-name'

# Consumer configuration
# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
conf = {
    'bootstrap.servers': servers,
    'group.id': group_id,
    'session.timeout.ms': 6000,
    'default.topic.config': {'auto.offset.reset': 'smallest'},
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'SCRAM-SHA-256',
    'sasl.username': username,
    'sasl.password': password,
}
p = Producer(conf)

try:
    p.produce(topic, "my message new 2", callback=delivery_callback)
except BufferError as e:
    print('%% Local producer queue is full (%d messages awaiting delivery): try again\n',
          len(p))
p.poll(0)

print('%% Waiting for %d deliveries\n' % len(p))
p.flush()
#+END_SRC

*** Consumers
Subscrevem a determinadas filas (tópicos) e continuamente consomem as mensagens geradas pelo Producer

#+BEGIN_SRC python
topics = ['topic-name']

# Consumer configuration
# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
conf = {
    'bootstrap.servers': servers,
    'group.id': group_id,
    'session.timeout.ms': 6000,
    'default.topic.config': {'auto.offset.reset': 'smallest'},
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'SCRAM-SHA-256',
    'sasl.username': username,
    'sasl.password': password,
}

c = Consumer(conf)
c.subscribe(topics)
try:
    while True:
        msg = c.poll(timeout=1.0)
        if msg is None:
            continue
        if msg.error():
            # Error or event
            if msg.error().code() == KafkaError._PARTITION_EOF:
                # End of partition event
                sys.stderr.write('%% %s [%d] reached end at offset %d\n' %
                                 (msg.topic(), msg.partition(), msg.offset()))
            elif msg.error():
                # Error
                raise KafkaException(msg.error())
        else:
            # Proper message
            sys.stderr.write('%% %s [%d] at offset %d with key %s:\n' %
                             (msg.topic(), msg.partition(), msg.offset(),
                              str(msg.key())))
            print(msg.value())

except KeyboardInterrupt:
    sys.stderr.write('%% Aborted by user\n')

# Close down consumer to commit final offsets.
c.close()
#+END_SRC

** Kafka as a Service
Karafka - Managed Apache Kafka Cluster
https://www.cloudkarafka.com/

5 tópicos grátis (free-tier)

** Recursos
https://github.com/huguinho-alves/python_aceleracao_everys
[[https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a][Start your real-time pipeline with Apache Kafka]]
[[https://stackoverflow.com/questions/4127241/orchestration-vs-choreography#:~:text=The%20choreography%20describes%20the%20interactions,the%20services%20involved%20should%20reside.][Orchestration vs. Choreography]]

* Próximas aulas
** Processando grandes conjuntos de dados de forma paralela e distribuída com Spark
** Criando pipelines de dados eficientes - Parte 1
** Criando pipelines de dados eficientes - Parte 2
** Orquestrando Big Data em Ambiente de Nuvem
** Scala: o poder de uma linguagem multiparadigma
** O que você precisa saber para construir APIs verdadeiramente restfull

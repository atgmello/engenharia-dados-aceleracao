:PROPERTIES:
:TOC:      :include all :depth 2
:END:
#+TITLE: Engenharia de Dados
:CONTENTS:
1. [[#abertura---como-trabalhar-em-um-ambiente-%C3%A1gil-e-global][Abertura - Como trabalhar em um ambiente ágil e global]]

2. [[#monitoramento-de-clusters-hadoop-de-alto-n%C3%ADvel-com-hdfs-e-yarn][Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn]]

3. [[#orquestrando-ambientes-de-big-data-distruibuidos-com-zookeeper-yarn-e-sqoop][Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop]]

4. [[#como-realizar-consultas-de-maneira-simples-no-ambiente-complexo-de-big-data-com-hive-e-impala][Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala]]

5. [[#trabalhando-com-servi%C3%A7os-de-mensageria-real-time-com-python-e-kafka-na-pr%C3%A1tica][Trabalhando com serviços de mensageria real time com Python e Kafka na prática]]

6. [[#processando-grandes-conjuntos-de-dados-de-forma-paralela-e-distribu%C3%ADda-com-spark][Processando grandes conjuntos de dados de forma paralela e distribuída com Spark]]

7. [[#pyspark-criando-pipelines-de-dados-eficientes][PySpark: Criando pipelines de dados eficientes]]

8. [[#orquestrando-big-data-em-ambiente-de-nuvem][Orquestrando Big Data em Ambiente de Nuvem]]

:END:

* Abertura - Como trabalhar em um ambiente ágil e global
** Introdução
Kick-off da Aceleração.  A live começou com a apresentação da [[https://www.everis.com/brazil/pt-br/home-br][Everis]], empresa de
consultoria multinacional que estará ministrando as palestras técnicas sobre o
assunto de Big Data. Entraram em detalhes sobre a filosofia e cultura da
empresa, bem como ressaltaram que há oportunidades na área de Dados e que
poderão ser oferecidas aos alunos de destaque do curso ao fim do treinamento.

Esse curso contou com um total de 2000 candidaturas. Por volta de 400 foram
selecionados após realizar uma prova eliminatória. A prova contou com questões
de multipla escolha sobre Python e lógica de programação, além de um coding
test.

Serão 14 dias de treinamento passando pelas linguagens de programação Python e
Scala além de ferramentas fundamentais para Big Data. Serão dadas noções do
_core_ Hadoop com HDFS, Yarn, além de outras ferramentas do ecossistema como
MapReduce, Sqoop, Hive, Zookeeper e Spark.  Também serão cobertas noções de
bancos de dados NoSQL como Cassandra e HBase.

*** Agile
Foi ressaltada a importância da mudança do paradigma Waterfall para o paradigma
Agile em projetos de Engenharia de Dados.  Planejamento e execução de Sprints,
Dailies, Reviews, organização em Squads autosuficientes, constante interação com
o cliente, foco na entrega. Essas são algumas das características essenciais da
cultura Agile e que contribuem para o sucesso de projetos utilizando esse
framework.

#+html: <p align="center"> <figure>
#+html: <img src="figuras/BIG_AgileProcess-V2.png" />
#+html: <figcaption>Framwork Agile. Créditos: <a href="https://bitsinglass.com/agile-methodologies-enhance-appian-delivery-part-1/">bitsinglass</a> </figcaption>
#+html: </figure> </p>

Agile requer um processo de comunicação forte, com transparência, confiânça e
união entre os colaboradores das Squads. A colaboração é um aspecto fundamental
para o sucesso no framework Agile.

*** Recursos
[[https://agilemanifesto.org/][Manifesto Agile]]

[[https://agilemanifesto.org/principles.html][Princípios Agile]]

[[https://medium.com/@WeAreMobile1st/the-benefits-of-teamwork-and-collaboration-in-software-development-a843cb7e8f73][The Benefits of Teamwork and Collaboration in Software Development]]

* Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn
** Sistemas distribuídos
Clusters de computadores hoje em dia são uma alternativa aos antigos mainframes.
Enquanto mainframes conseguem oferecer escalabilidade vertical, cluters
conseguem oferecer escalabilidade horizontal. Isso é, para conseguir mais
recursos de armazenamento e computação para os mainframes você precisa

Nesse contexto, demos início ao estudo do Hadoop, uma tecnologia desenvolvida
utilizando princípios de sistemas distribuídos e que, portanto, se encaixam
nessa nova realidade dos clusters. Passamos por uma descrição alto nível do que
acontece por debaixo dos panos quando nós, desenvolvedores, interagimos com o
HDFS e Yarn.

** Hadoop
Essas duas tecnologias são as peças fundamentais que sustentam o ecossistema
Hadoop. Os nomes HDFS e Yarn podem parecer uma sopa de letrinhas sem muito
sentido para quem os vê fora de contexto. Mas note que ambos na verdade são
siglas bastante descritivas do que essas tecnologias se propõem a fazer.

O HDFS, ou Hadoop Distributed File System, fica encarregado de distribuir e
gerenciar os seus arquivos entre as máquinas do seu cluster. Já o Yarn, Yet
Another Resource Negotiator, é responsável por negociar recrusos computacionais
para as suas tarefas de manipulação de dados.

#+html: <figure align="center">
#+html: <img src="figuras/hadoop_eco.png" />
#+html: <figcaption>Ecossistema Hadoop. Créditos: <a href="https://dzone.com/articles/example-of-etl-application-using-apache-spark-and">dzone</a> </figcaption>
#+html: </figure>

* Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop
** Zookeepr
Serviço de coordenação distribuído.
Fornece as rotas necessárias para as peças do cluster. Identifica nós por nomes (DSN-like).
Ajuda as peças do ecossitemas Hadoop se achar na bagunça toda.
Pode ajudar a evitar concorrência (write on read).
Ajuda na recuperação de falhas.

Para o desenvolvedor, o Zookeeper é completamente transparente.
É mais aparente para quem vai trabalhar com infra.

** Sqoop
Movimenta dados entre banco de dados relacional e HDFS.  Realiza a leitura linha
a linha de tabelas para arquivos (pode pecar na performance).  Permite importar
dados e metadados de bancos de dado relacionais direto para o Hive.  Utiliza
MapReduce por debaixo dos panos: processamento paralelo e tolerante a falha.
Contudo MapReduce já não é mais o motor mais rápido disponível (alternativas:
Spark, Flink).

#+BEGIN_SRC bash
sqoop import \
    --connect jdbc: \
    --username abc \
    --password xyz \
    --table some_table \
    --where "column='something'"
#+END_SRC

*** Desafio
**** SQL
1. Todos os Pokémon lendários
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE legendary=TRUE;
   #+END_SRC
2. Todos os Pokémon de apenas um tipo
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE type2='';
   #+END_SRC
3. Os top 10 Pokémon mais rápidos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY speed DESC
    LIMIT 10;
   #+END_SRC
4. Os top 50 Pokémon com menos HP
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY hp ASC
    LIMIT 50;
   #+END_SRC
5. Os top 100 Pokémon com maiores atributos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY
    greatest(hp, attack, defense, spatk, spdef, speed)
    DESC
    LIMIT 100;
   #+END_SRC

**** Sqoop

** Recursos
http://dontpad.com/aceleracaoeveris
https://drive.google.com/drive/folders/1xaft6H3R3_UvA6-BFHuCvHuWczf6xwqG?usp=sharing

* Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala
Hive e Impala são frontends que possibilitam um a conexão facilitada aos dados no HDFS.

** Hive
Hive oferece a linguagem HQL (Hive Query Langue), uma abstração de alto nível ao MapReduce, com linguagem similar ao conhecido SQL.

Três possíveis engines MapReduce, Spark, Tez.
** Impala
Engine MPP (Massive Parallel Processing). Também uma linguagem similar a SQL (Impala SQL).

Impala não salva os resultados intermediários em disco. Isso acelera e muito o
processamento em comparação com o Hive.

** Diferenças
Casos de uso:
Realtime -> Impala
Batch -> Hive

Impala não salva intermediários em disco, fica tudo em memória. Portanto seu
consumo de memória é muito maior, o que pode ser um limitante dependendo do tipo
de queries executadas.

** Detalhes HQL
CREATE EXTERNAL vs MANAGED TABLE
External - Quando a tablea é apagada, os dados permanecem
Managed - Apaga os dados quando a tabela é deletada

** Formatos de arquivos
*** Parquet
Formato Colunar
*** ORC
Formato Colunar
*** Avro
Formato de Linhas

** Particionamento
Determina como os dados são armazenados.

Pouco particionamento: não faz bom uso da capacidade de paralelismo dos dados
Muito particionamento: pode sobrecarregar o namenode, impactando na performance

** Prática
*** Hive
Utilitários
#+BEGIN_SRC sql
set hive.cli.print.header=true;
set hive.cli.print.current.db=true;
#+END_SRC

Criar tabela em cima de pasta do HDFS.
#+BEGIN_SRC sql
CREATE EXTERNAL TABLE TB_EXT_EMPLOYEE(
id STRING,
groups STRING,
age STRING,
active_lifestyle STRING,
salary STRING)
ROW FORMAT DELIMITED FIELDS
TERMINATED BY '\;'
STORED AS TEXTFILE
LOCATION '/user/hive/warehouse/external/tabelas/employee'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Enviar dados para a LOCATION especificada pela tabela acima.
#+BEGIN_SRC sql
hdfs dfs -put /home/everis/employee.txt /user/hive/warehouse/external/tabelas/employee
#+END_SRC

Melhorar tabela acima com os tipos apropriados.
#+BEGIN_SRC sql
CREATE TABLE TB_EMPLOYEE(
id INT,
groups STRING,
age INT,
active_lifestyle STRING,
salary DOUBLE)
PARTITIONED BY (dt_processamento STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS PARQUET TBLPROPERTIES ("parquet.compression"="SNAPPY");

insert into table TB_EMPLOYEE partition (dt_processamento='20201118') 
select
id,
groups,
age,
active_lifestyle,
salary
from TB_EXT_EMPLOYEE;
#+END_SRC

Criar tabela em cima de pasta no HDFS (segundo exemplo)
#+BEGIN_SRC sql
create external table localidade(
street string,
city string,
zip string,
state string,
beds string,
baths string,
sq_ft string,
type string,
sale_date string,
price string,
latitude string,
longitude string)
PARTITIONED BY (particao STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ","
STORED AS TEXTFILE
location '/user/hive/warehouse/external/tabelas/localidade'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Alternativamente, criar tabela com base em arquivo.
Hive envia dados para o HDFS automaticamente.
#+BEGIN_SRC sql
load data local inpath '/home/everis/base_localidade.csv' 
into table teste.localidade partition (particao='2021-01-21');
#+END_SRC

**** Join
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins

** Dia-a-dia
/home/cloudera/hive/script.sh
#+BEGIN_SRC shell
#!/bin/bash

dt_processamento=$(date '+%Y-%m-%d')
path_file='/home/cloudera/hive/datasets/employee.txt'
table=beca.ext_p_employee
load=/home/cloudera/hive/load.hql

hive -hiveconf dt_processamento=${dt_processamento} -hiveconf table=${table} -hiveconf path_file=${path_file} -f $load 2>> log.txt

hive_status=$?

if [ ${hive_status} -eq 0 ];
then
        echo -e "\nScript executado com sucesso"
else
        echo -e "\nHouve um erro na ingestao do arquivo "

impala-shell -q 'INVALIDATE METADATA beca.ext_p_employee;'

fi
#+END_SRC

/home/cloudera/hive/load.hql
#+BEGIN_SRC shell
LOAD DATA LOCAL INPATH '${hiveconf:path_file}' INTO TABLE ${hiveconf:table} PARTITION(dt_processamento='${hiveconf:dt_processamento}');
#+END_SRC

** Recursos
https://gitlab.com/vmb1/hive

* Explorando o poder do NoSQL com Cassandra e Hbase
Por quê NoSQL em Big Data? Resposta: Performance.

MapReduce processa dados em batch e os dados são acessados de forma sequencial. Ou seja, é preciso percorrer todo o dataset (scan search), mesmo para jobs mais simples.

NoSQL possibilita acesso aleatório aos dados em termos de posição do registro e tempo.

** CAP
Teorema CAP: Consistência, Disponibilidade, Partição Tolerante a Falhas.

** HBase
Banco de dados distribuído e orientado a coluna (Column Family ou Wide Column).

É um Map:
- Esparso
- Distribuído
- Persistente
- Multidimensional
- Ordenado

Depende do Zookeeper para que consiga funcionar. O Zookeeper dá ao HBase visibilidade a todos os nós do cluster.

**** Desvantagem
- Infelizmente não possui uma linguagem de busca (query)
- Não suporta índices em colunas fora da rowkey
- Não suporta tabelas secundárias de índices

**** Vantagem
- Fácil integração ao ecossistema Hadoop

*** Estrutura
- Map indexado por uma linha chave (row key), coluna chave (column key) e uma coluna timestamp.
- Cada valor no Map é interpretado como um vetor de bytes (array of bytes)
- Não distingue tipos (int, str, etc): pode armazenar qualquer tipo de dado, inclusive documentos (JSON, CSV, ...)

{

}

*** Arquitetura
*** Exemplos
Criação de tabela
#+BEGIN_SRC bash

#+END_SRC

Inserção de dados
#+BEGIN_SRC bash

#+END_SRC

Versionamento
#+BEGIN_SRC bash

#+END_SRC

Deleção
#+BEGIN_SRC bash

#+END_SRC

Deleção em coluna versionada
#+BEGIN_SRC bash

#+END_SRC

TTL
#+BEGIN_SRC bash
create 'ttl_exemplo', {'NAME'=>'cf', 'TTL'=>20}
put 'ttl_exemplo', '1', 'cf:nome', 'Informacao'
scan 'ttl_exemplo'
#+END_SRC

** Cassandra
Banco de dados distribuído e orientado a coluna (Column Family ou Wide Column).

Os dados aqui são tipados.

Possui linguagem CQL (SQL-like), porém com algumas operações não suportadas/recomendadas (eg joins, alguns tipos de agrupamento e filto).

Suporta tabela secundárias de índices e filtros em colunas fora da primary key.

*** Arquitetura
Não possui ponto de falha central.

Conexão entre nós é realizada de ponta a ponta, utilizando o protocolo Gossip para distribuição dos dados.

Commit table -> memtable -> SSTable

** TTL
Registro temporário: colunas com propriedade TTL - Time To Live. Os registros são apagados depois desse período.

** Cenários de uso
- Arquitetura baseada em eventos
    Kafka Cluster -> Spark Streaming -> HBase/Cassandra -> Enriquecimento -> Destino

** Recursos
https://github.com/pentguard/DIO-Aceleracao-4-HBase-Cassandra

https://www.datastax.com/

** Etc
#+BEGIN_SRC bash
sudo -u hdfs hadoop dfsadmin -safemode leave
#+END_SRC

Por que o safemode é ativado? Arquivos corrompidos.

É importante parar todos os serviços do Hadoop antes de desligar a máquina.

Como resolver: recuperar os arquivos (caso haja replicação) ou apagar.

* Intensivo de Python: O mínimo que você precisa saber
** Recursos
https://github.com/huguinho-alves/python_aceleracao_everys

* Trabalhando com serviços de mensageria real time com Python e Kafka na prática
** Introdução a Micro Serviços
Existem diferentes tipos de arquiteturas.
*** Monolito
Ex: Banco + Aplicação (front + back) tudo no mesmo servidor.
**** Desvantagens
- Forte acoplamento entre diferentes módulos e responsabilidades
**** Vantagens
- Comunicação simplificada entre componentes do sistema
*** Micro Serviços
O sistema é dividido em diversos "módulos" (diversas partes/serviços).
**** Desvantagens
- Complexidade em coordenar a comunicação de diferentes servidores/serviços
- Comunicação pode gerar acoplamento entre os módulos
  - Contudo, esse ponto pode ser solucionado através de um serviço de mensageria!
**** Vantagens
- Isolamento de responsabilidades
  - Melhor manutenibilidade/sustentação
- Controle granulirizado da escala de cada um dos serviços
  - Escalabilidade facilitada

*** Conclusão
[[http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html][Não existe almoço grátis.]]

Talvez um ponto controverso, mas que vale a pena ser considerado: [[https://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html][micro serviços não são um tipo de arquitetura.]]

** Kafka
Sistema de mensageria Open Source.

#+BEGIN_QUOTE
Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.
#+END_QUOTE

#+html: <p align="center"> <figure>
#+html: <img src="figuras/kafka_ex.png" />
#+html: <figcaption>Visão geral dos componentes Kafka. Créditos: <a href="https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a">kavimaluskam@medium</a> </figcaption>
#+html: </figure> </p>

*** Producers
Geram mensagens, que são enviadas para uma fila (mais especificamente tópicos, no Kafka), a serem lidas por Consumers.

#+BEGIN_SRC python
topic = 'topic-name'

# Consumer configuration
# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
conf = {
    'bootstrap.servers': servers,
    'group.id': group_id,
    'session.timeout.ms': 6000,
    'default.topic.config': {'auto.offset.reset': 'smallest'},
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'SCRAM-SHA-256',
    'sasl.username': username,
    'sasl.password': password,
}
p = Producer(conf)

try:
    p.produce(topic, "my message new 2", callback=delivery_callback)
except BufferError as e:
    print('%% Local producer queue is full (%d messages awaiting delivery): try again\n',
          len(p))
p.poll(0)

print('%% Waiting for %d deliveries\n' % len(p))
p.flush()
#+END_SRC

*** Consumers
Subscrevem a determinadas filas (tópicos) e continuamente consomem as mensagens geradas pelo Producer

#+BEGIN_SRC python
topics = ['topic-name']

# Consumer configuration
# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
conf = {
    'bootstrap.servers': servers,
    'group.id': group_id,
    'session.timeout.ms': 6000,
    'default.topic.config': {'auto.offset.reset': 'smallest'},
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'SCRAM-SHA-256',
    'sasl.username': username,
    'sasl.password': password,
}

c = Consumer(conf)
c.subscribe(topics)
try:
    while True:
        msg = c.poll(timeout=1.0)
        if msg is None:
            continue
        if msg.error():
            # Error or event
            if msg.error().code() == KafkaError._PARTITION_EOF:
                # End of partition event
                sys.stderr.write('%% %s [%d] reached end at offset %d\n' %
                                 (msg.topic(), msg.partition(), msg.offset()))
            elif msg.error():
                # Error
                raise KafkaException(msg.error())
        else:
            # Proper message
            sys.stderr.write('%% %s [%d] at offset %d with key %s:\n' %
                             (msg.topic(), msg.partition(), msg.offset(),
                              str(msg.key())))
            print(msg.value())

except KeyboardInterrupt:
    sys.stderr.write('%% Aborted by user\n')

# Close down consumer to commit final offsets.
c.close()
#+END_SRC

** Kafka as a Service
Karafka - Managed Apache Kafka Cluster

https://www.cloudkarafka.com/

5 tópicos grátis (free-tier)

** Recursos
https://github.com/huguinho-alves/python_aceleracao_everys

[[https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a][Start your real-time pipeline with Apache Kafka]]

[[https://stackoverflow.com/questions/4127241/orchestration-vs-choreography#:~:text=The%20choreography%20describes%20the%20interactions,the%20services%20involved%20should%20reside.][Orchestration vs. Choreography]]

[[http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html][Microservices - Not A Free Lunch!]]

[[https://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html][Clean Micro-service Architecture]]

* Processando grandes conjuntos de dados de forma paralela e distribuída com Spark
** O que é Spark?
#+BEGIN_QUOTE
Apache Spark is a unified analytics engine for large-scale data processing.
#+END_QUOTE

https://spark.apache.org/

** Databricks
Distribuição comercial do Apache Spark.

#+BEGIN_QUOTE
Databricks adds enterprise-grade functionality to the innovations of the open source community. As a fully managed cloud service, we handle your data security and software reliability.
#+END_QUOTE

https://databricks.com/

** Diferencial do Spark
Executa processamento de dados até 100 vezes mais rápido que o MapReduce tradicional.

#+BEGIN_QUOTE
They used Spark and sorted 100TB of data using 206 EC2 i2.8xlarge machines in 23 minutes. The previous world record was 72 minutes, set by a Hadoop MapReduce cluster of 2100 nodes.
#+END_QUOTE

https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html

*** O truque
Velocidade de acesso:

Cache > RAM > Disco

#+html: <p align="center"> <figure>
#+html: <img src="figuras/memory_speed.png" />

#+html: <figcaption>Tipos de memória. Créditos: <a href="https://medium.com/@esmerycornielle/the-cpu-and-the-memory-2eb300d6c72d">esmerycornielle@medium</a></figcaption>
#+html: </figure> </p>

Spark, diferentemente do MapReduce, traz os dados para RAM. Isso diminui consideravelmente o tempo de transformação dos dados.

Trata-se, portanto, de um framework in-memory.

*** Linguagens suportadas
- Scala
- Java
- Python
- R
- SQL

** Arquitetura
#+html: <p align="center"> <figure>
#+html: <img src="figuras/spark_overview.png" />
#+html: <figcaption>Arquitetura do Apache Spark. Créditos: <a href="https://spark.apache.org/docs/2.0.0/cluster-overview.html">Documentação</a></figcaption>
#+html: </figure> </p>

Driver Node contém o Spark Context, responsável por gerir os recursos a serem utilizados no processamento dos dados

- Worker Node = um nó (máquina) do cluster

- Cada Worker Node pode conter um ou mais Executors

- Cada Executor processa uma ou mais Tasks

- Executors avisam ao Spark Context o progresso de suas Tasks

  - Caso um Executor deixe de responder (máquina caiu) o Spark Context consegue criar novos Executores em outro Worker Node para resumir o processamento interrompido

*** Divisão
Apache Spark pode ser quebrado em cinco bibliotecas:

- SparkSQL
  Processamento de dados tabulares
- Spark Streaming
  Micro-batch de dados
- MLlib
  Machine Learning
- GraphX
  Dados em grafos

Todos são sustentados pelo Spark Core

** RDD
Resilient Distributed Dataset é a principal abstração do Spark.

- Resilient: Dado pode ser recuperado facilmente em caso de falhas

- Distributed: Pode ser processado por diferentes máquinas

RDDs são imutáveis. Transformações sobre RDDs geram novas RDDs.
** Exemplos
- Context
  #+BEGIN_SRC scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName("Simple Application")
val sc = new SparkContext(conf)
  #+END_SRC

- Leitura de dados
  #+BEGIN_SRC scala
val df = spark.read.format("csv").option("sep",",").option("header","true").load("file:///home/everis/avengers.csv")

insurance.show(10, false)

val df_url = insurance.select("URL")
  #+END_SRC

- Manipulação
  #+BEGIN_SRC scala
df.select("field1","field2").show()
df.select($"field1", $"field2"+1).show()
df.groupBy("age").count().show()
  #+END_SRC

- SQL
  #+BEGIN_SRC scala
df.createOrReplaceTempView("av")

spark.sql("SELECT Appearances FROM av where URL LIKE '%Iron_Man%'").show()
  #+END_SRC

  - UDF
    User Defined Function
    #+BEGIN_SRC scala
val squared = (s: Long) => {
  s * s
}
spark.udf.register("square", squared)

spark.sql("SELECT square(Appearances) FROM av").show()
    #+END_SRC

** Recursos
[[https://spark.apache.org/][Spark]]

[[https://databricks.com/][Databrick]]

[[https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html][Spark wins Daytona Gray Sort 100TB Benchmark]]

[[http://spark.apache.org/docs/2.4.0/cluster-overview.html][Cluster Mode Overview]]

[[https://docs.microsoft.com/pt-br/azure/databricks/spark/latest/spark-sql/udf-scala][Funções definidas pelo usuário – Scala]]

[[https://medium.com/@esmerycornielle/the-cpu-and-the-memory-2eb300d6c72d][The CPU and The Memory]]

[[https://www.udacity.com/course/learn-spark-at-udacity--ud2002][Learn Spark at Udacity (Free)]]

[[https://github.com/databricks/koalas][Koalas (Pandas from Spark)]]

* PySpark: Criando pipelines de dados eficientes
** Aplicabilidade
- Operações de extração-transformação-carregamento (ETL)
- Análise preditiva e aprendizado de máquina
  - Reconhecimento de padrões
  - Mecanismos de recomendação
- Operações de acesso a dados (como consultas e visualização SQL)
- Mineração e processamento de texto
- Processamento de eventos em tempo real
- Análises de grafos

** Arquitetura
*** Driver
O Driver é responsável por criar o SparkContext, também referico como sc. É instanciado no início de um aplicativo Spark (e shells) e é usado durante todo o programa.

*** Cluster Manager
resource_manager:8088

*** Spark Master

** Uso
#+BEGIN_SRC bash
spark submit \
    --master yarn \
    --queue "SquadFi" \
    --name "Program Name" \
    --driver-memory 2G \
    --executor-memory 2G \
    --executor-cores 1 \
    --proxy-user hive \
    --conf "spark.driver.maxResultSize=16g" \
    --conf "spark.dynamicAllocation.enabled=true" \
    --conf "spark.shuffle.service.enabled=true" \
    --conf "spark.shuffle.service.port=7337" \
    --conf "spark.dynamicAllocation.initialExecutors=10" \
    --conf "spark.dynamicAllocation.minExecutors=10" \
    --conf "spark.dynamicAllocation.maxExecutors=10" \
    --conf "spark.yarn.driver.memoryOverhead=2000" \
    --conf "spark.yarn.executor.memoryOverhead=2000" \
    --driver-java-options "-Djavax.security.auth.useSubjectCredsOnly=false" \
    Main.py <param1> <param2> <...> <paramN>
#+END_SRC

*** Context
**** HiveContext
#+BEGIN_SRC python
from pyspark import SparkContext, HiveContext
sc = SparkContext(appName = "test")
sqlContext = HiveContext(sc)
sqlContext.sql("select * from SomeTable limit 10")
#+END_SRC

**** StreamContext
#+BEGIN_SRC python
from pyspark.streaming import StreamingContext

ssc = StreamingContext(sc, 1)
lines = ssc.socketTextStream('localhost', 9999)
counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
counts.pprint()
ssc.start()
ssc.awaitTermination()
#+END_SRC

***** Caso de Uso
Exemplo:
#+html: <p align="center"> <figure>
#+html: <img src="figuras/apache_spark_streaming.png" />
#+html: <figcaption>Arquitetura do Apache Spark. Créditos: <a href="https://databricks.com/glossary/what-is-spark-streaming">Documentação</a></figcaption>
#+html: </figure> </p>

** Distribuições
*** On Premisses
**** Cloudera
[[https://www.cloudera.com/][Cloudera]]
*** Cloud
**** AWS
- EMR
  - Managed Hadoop Framework (Hive, Zookeeper, Sqoop, Oozie, Tez, Hue, Phoenix, Spark, Presto, ...)
  - Forma simples e automatizada de subir cluster com ecossistema Hadoop
    - Evita o trabalho de subir diversas máquinas EC2, configurar rede, configurar cada uma das máquinas e os diversos aplicativos...
- AWS Glue DataBrew
  - Visual data preparation tool to clean and normalize data for analytics and machine learning
  - É um Spark visual e serverless

**** Databricks
[[https://community.cloud.databricks.com/login.html][Databricks Community]]

** Pitfalls
- Particionamento dos dados no HDFS
  - Inserir arquivos muito pequenos acaba desperdiçando espaço: o HDFS possui um tamanho fixo para cada bloco, independente do tamanho do arquivo
  - Dados divididos em muitods pedaços impactam negativamente na velocidade da recuperação de dados
** Boas práticas
*** Camadas
- Raw Zone
  Todo dado recebido pelos sistemas origens classificamos como Raw Zone.

  Em geral, todo o dado chega em formato String, o mais tipo mais flexível e genérico de todos. Assim garantimos que o dado seja armazenado AS-IS.

- Trusted Zone
  Na camada Trusted realizamos ingestões mais direcionadas a regras de negócio.

  Aqui são realizados cruzamentos de tabelas Raw, bem como Cast dos dados para os tipos ideais, além da aplicação de regras de negócio.

- Refined Zone
  Na última camada, Refined, criamos visões para relatórios.

  Aqui aparecem apenas as colunas necessárias para as visões esperadas.

** Tipos de armazenamento
- Avro
- Parquet
- ORC

#+html: <p align="center"> <figure>
#+html: <img src="figuras/file_formats.png" />
#+html: <figcaption>Tipos de formatos de dados para Big Data. Créditos: <a href="https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/">Datanami</a></figcaption>
#+html: </figure> </p>

** Recursos
[[https://docs.cloudera.com/runtime/7.2.1/spark-overview/topics/spark-overview.html][Spark Overview for Data Science]]

[[https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/][Spark: The Definitive Guide]]

[[https://academy.databricks.com/exam/databricks-certified-associate-developer][Databricks Certified Associate Developer for Apache Spark 3.0]]

[[https://www.cloudera.com/about/training/certification/cca-spark.html][CCA Spark and Hadoop Developer Exam (CCA175)]]

[[https://www.cloudera.com/about/training/certification/ccp-data-engineer.html][CCP Data Engineer Exam (DE575)]]

[[https://www.kaggle.com/gpreda/covid-world-vaccination-progress][Covid World Vaccination Dataset (Kaggle)]]

[[https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5191291087587071/3239850342304675/2520349622255950/latest.html][Notebook]]

* Orquestrando Big Data em Ambiente de Nuvem
** Cloud Computing
*** Níveis gerais
- Infrastructure as a Service (IaaS)
- Platform as a Service (PaaS)
- Software as a Service (SaaS)

#+html: <p align="center"> <figure>
#+html: <img src="figuras/cloud_computing.png" />
#+html: <figcaption>Níveis de Cloud Computing. Créditos: <a href="https://www.ibm.com/blogs/cloud-computing/2014/01/31/cloud-computing-defined-characteristics-service-levels/">IBM</a></figcaption>
#+html: </figure> </p>

**** Detalhamento

#+html: <p align="center"> <figure>
#+html: <img src="figuras/cloud_computing_detailed.webp" />
#+html: <figcaption>Níveis de Cloud Computing, detalhamento. Créditos: <a href="https://imelgrat.me/cloud/cloud-services-models-help-business/">Imelgrat</a></figcaption>
#+html: </figure> </p>

** Amazon
Overview dos principais serviços da AWS.

#+html: <p align="center"> <figure>
#+html: <img src="figuras/gartner_aws.jpg" />
#+html: <figcaption>Gartner Magic Quadrant. Créditos: <a href="https://aws.amazon.com/blogs/aws/aws-named-as-a-leader-in-gartners-infrastructure-as-a-service-iaas-magic-quadrant-for-the-9th-consecutiveyear/">Amazon</a></figcaption>
#+html: </figure> </p>

*** Amazon Compute Services
- EC2
  - Elastic Compute Cloud
- Lambda
  - Serverless Computing (Functions)
- ECS
  - Elastic Container Service
- EKS
  - Elastic Kubernetes Service
- Fargate
  - Serverless Containers

*** Amazon Network Services
- VPC
  - Virtual Private Cloud
- Cloud Front
  - CDN (Content Delivery Network)
- API Gateway
- Route53
  - DNS (Domain Name Service)
- VPN
  - Virtual Private Network
- Direct Connect
  - Alternativa ao VPN

*** Amazon Storage Services
- S3
  - Simple Storage Service
- Glacier
  - S3 de baixo acesso e longa duração
- EBS
  - Elastic Block Store
  - Mantém os dados armazenados próximos à computação
- EFS
  - Elastic File System (NFS da AWS)

*** Amazon Security Services
- IAM
  - Identity and Access Management
- ACM
  - AWS Certificate Manager
- KMS
  - Key Management Service
- WAF
- Inspector
  + Avaliação automática de segurança
- CloudHSM
  - Hardware Security Management

*** Amazon Management Services
- CloudWatch
  - Coleta dados de monitoramento
- CloudTrail
  - Governança e rastreabilidade de usuários
- Systems Manager
  - Simplifica o gerenciamento de recursos
- Trusted Advisor
  - Dicas de melhores práticas em real-time

*** Amazon DevOps Services
- CodeCommmit
  + Git da AWS
- CodePipeline
- CodeBuild
- CodeDeploy
- CloudFormation
  + IaC (Infrastructure as Code)
- OpsWork
  + IaC com Chef e Puppet
- Config
  + Auditoria de configurações
- Service Catalog

*** Amazon Application Services
- Storage Gateway
- Cloud Search
  + Elasticsearch da AWS
- SQS
  + Simple Queue Service
  + RabbitMQ da AWS
- SES
  + Simple Email Service
- SWF
  + Simple Workflow Service

*** Amazon Mobile Services
- Cognito
  + Autenticação e gerenciamento de usuários de apps
- SNS
  + Simple Notification Service
- Mobile Analytics
  + Amazon Pinpoint
- Device Farm
  + Serviço de teste de aplicativos
- Mobile Hub
  + Helper de configuração de serviços back-end para apps móveis

*** Amazon Database Services
- RDS
  + Relation Database Service
- DynamoDB
  + HBase (NoSQL) da AWS
- Redshift
  + Datawarehouse
  + PostgreSQL distribuído da AWS
- ElastiCache
  + In-memory Database
  + Redis da AWS

*** Amazon Analytics Services
- Athena
  + Presto da AWS
- EMR
  + Elastic MapReduce
- Data Pipeline
  + Diversas tecnologias de ETL
- Glue
  + Similar ao Data Pipeline, mas foco em big data
- Kinesis
  + Kafka da AWS
- QuickSight
  + Tableau da AWS
- Elasticsearch Service
  + Elasticsearch as a service

*** Amazon Machine Learning
- SageMaker
  + JupyterLab da AWS
- Comprehend
  + NLP as a service
- Polly
  + Text to Speech (TTS)
- Lex
  + Chatbot
- Rekognition
  + Visão Computacional (imagens) as a service
- DeepLens
  + Visão Computacional (vídeo) as a service
- Transcribe
  + Automatic Speech Recognition

** Recursos
[[https://www.ibm.com/blogs/cloud-computing/2014/01/31/cloud-computing-defined-characteristics-service-levels/][Cloud computing defined: Characteristics & service levels]]

[[https://imelgrat.me/cloud/cloud-services-models-help-business/][Cloud services delivery models. Which can help your business?]]

[[https://aws.amazon.com/blogs/aws/aws-named-as-a-leader-in-gartners-infrastructure-as-a-service-iaas-magic-quadrant-for-the-9th-consecutiveyear/][AWS Named as a Leader in Gartner’s Infrastructure as a Service (IaaS) Magic Quadrant for the 9th Consecutive Year]]

* Scala: o poder de uma linguagem multiparadigma
Hands-on Scala.

** Criando pacotes
- Maven

#+BEGIN_QUOTE
Apache Maven, ou Maven, é uma ferramenta de automação de compilação utilizada
primariamente em projetos Java.
#+END_QUOTE

- Alternativas:
  + SBT
  + Gradle
  + Ant

** Sintaxe
Recursos interessantes presentes na linguagem.

*** val vs var
#+BEGIN_SRC scala
// Declaring values is done using either var or val.
// val declarations are immutable, whereas vars are mutable. Immutability is
// a good thing.
val x = 10 // x is now 10
x = 20     // error: reassignment to val
var y = 10
y = 20     // y is now 20
#+END_SRC

*** Pattern matching
#+BEGIN_SRC scala
def matchEverything(obj: Any): String = obj match {
  // You can match values:
  case "Hello world" => "Got the string Hello world"

  // You can match by type:
  case x: Double => "Got a Double: " + x

  // You can specify conditions:
  case x: Int if x > 10000 => "Got a pretty big number!"

  // You can match case classes as before:
  case Person(name, number) => s"Got contact info for $name!"

  // You can match regular expressions:
  case email(name, domain) => s"Got email address $name@$domain"

  // You can match tuples:
  case (a: Int, b: Double, c: String) => s"Got a tuple: $a, $b, $c"

  // You can match data structures:
  case List(1, b, c) => s"Got a list with three elements and starts with 1: 1, $b, $c"

  // You can nest patterns:
  case List(List((1, 2, "YAY"))) => "Got a list of list of tuple"

  // Match any case (default) if all previous haven't matched
  case _ => "Got unknown object"
}
#+END_SRC

*** Iterando sobre iterables
#+BEGIN_SRC scala
// Certain collections (such as List) in Scala have a `foreach` method,
// which takes as an argument a type returning Unit - that is, a void method
val aListOfNumbers = List(1, 2, 3, 4, 10, 20, 100)
aListOfNumbers foreach (x => println(x))
aListOfNumbers foreach println
#+END_SRC

*** for comprehension
#+BEGIN_SRC scala
for { n <- s } yield sq(n)

val nSquared2 = for { n <- s } yield sq(n)

for { n <- nSquared2 if n < 10 } yield n

for { n <- s; nSquared = n * n if nSquared < 10} yield nSquared

/* NB Those were not for loops. The semantics of a for loop is 'repeat', whereas
   a for-comprehension defines a relationship between two sets of data. */
#+END_SRC

*** map
#+BEGIN_SRC scala
val add10: Int => Int = _ + 10 // A function taking an Int and returning an Int
List(1, 2, 3) map add10 // List(11, 12, 13) - add10 is applied to each element

// Anonymous functions can be used instead of named functions:
List(1, 2, 3) map (x => x + 10)

// And the underscore symbol, can be used if there is just one argument to the
// anonymous function. It gets bound as the variable
List(1, 2, 3) map (_ + 10)
#+END_SRC

*** reduce
#+BEGIN_SRC scala
val listaReduceLeft = List[Int](5, 10, 15, 20)

val listaReduced = listaReduceLeft.reduce((val1, val2) => val1 + val2)

val listaOutraReduced = listaReduceLeft.reduce(_+_)
#+END_SRC

*** Tipos
Scala é uma linguagem fortemente tipada.
Porém não é necessário especificar os tipos das variáveis quando estiverem sendo criadas. O compilador consegue inferí-las.

#+html: <p align="center"> <figure>
#+html: <img src="figuras/scala_types.png" />
#+html: <figcaption>Scala Types. Créditos: <a href="http://scalajp.github.io/scala.github.com/tutorials/tour/unified-types.html">scalajp</a> </figcaption>
#+html: </figure> </p>

** Recursos
[[https://www.hammerlab.org/2017/04/06/scala-build-tools/][Building Scala Projects: Maven vs. SBT]]

[[https://scalameta.org/metals/][Metals]]

[[https://learnxinyminutes.com/docs/scala/][Learn X in Y minutes]]

[[http://scalajp.github.io/scala.github.com/tutorials/tour/unified-types.html][Unified Types]]

[[http://alvinalexander.com/scala][Alvin Alexander]]

* O que você precisa saber para construir APIs verdadeiramente restfull
** Recursos
[[https://www.redhat.com/pt-br/topics/api/what-is-a-rest-api][O que é API REST?]]

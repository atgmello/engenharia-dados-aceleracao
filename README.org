:PROPERTIES:
:TOC:      :include all :depth 2
:END:
#+TITLE: Engenharia de Dados
:CONTENTS:
1. [[#abertura---como-trabalhar-em-um-ambiente-%C3%A1gil-e-global][Abertura - Como trabalhar em um ambiente ágil e global]]

1. [[#monitoramento-de-clusters-hadoop-de-alto-n%C3%ADvel-com-hdfs-e-yarn][Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn]]

1. [[#orquestrando-ambientes-de-big-data-distruibuidos-com-zookeeper-yarn-e-sqoop][Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop]]

1. [[#como-realizar-consultas-de-maneira-simples-no-ambiente-complexo-de-big-data-com-hive-e-impala][Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala]]

1. [[#trabalhando-com-servi%C3%A7os-de-mensageria-real-time-com-python-e-kafka-na-pr%C3%A1tica][Trabalhando com serviços de mensageria real time com Python e Kafka na prática]]
:END:

* Abertura - Como trabalhar em um ambiente ágil e global
Kick-off da Aceleração.  A live começou com a apresentação da Everis, empresa de
consultoria multinacional que estará ministrando as palestras técnicas sobre o
assunto de Big Data. Entraram em detalhes sobre a filosofia e cultura da
empresa, bem como ressaltaram que há oportunidades na área de Dados e que
poderão ser oferecidas aos alunos de destaque do curso ao fim do treinamento.

Esse curso contou com um total de 2000 candidaturas. Por volta de 400 foram
selecionados após realizar uma prova eliminatória. A prova contou com questões
de multipla escolha sobre Python e lógica de programação, além de um coding
test.

Serão 14 dias de treinamento passando pelas linguagens de programação Python e
Scala além de ferramentas fundamentais para Big Data. Serão dadas noções do
_core_ Hadoop com HDFS, Yarn, além de outras ferramentas do ecossistema como
MapReduce, Sqoop, Hive, Zookeeper e Spark.  Também serão cobertas noções de
bancos de dados NoSQL como Cassandra e HBase.

Foi ressaltada a importância da mudança do paradigma Waterfall para o paradigma
Agile em projetos de Engenharia de Dados.  Planejamento e execução de Sprints,
Dailies, Reviews, organização em Squads autosuficientes, constante interação com
o cliente, foco na entrega. Essas são algumas das características essenciais da
cultura Agile e que contribuem para o sucesso de projetos utilizando esse
framework.

#+html: <p align="center"> <figure>
#+html: <img src="figuras/BIG_AgileProcess-V2.png" />
#+html: <figcaption>Framwork Agile. Créditos: <a href="https://bitsinglass.com/agile-methodologies-enhance-appian-delivery-part-1/">bitsinglass</a> </figcaption>
#+html: </figure> </p>

Agile requer um processo de comunicação forte, com transparência, confiânça e
união entre os colaboradores das Squads. No geral, focaram no fato de que
colaboração é um aspecto fundamental do framework Agile.

* Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn
** Sistemas distribuídos
Clusters de computadores hoje em dia são uma alternativa aos antigos mainframes.
Enquanto mainframes conseguem oferecer escalabilidade vertical, cluters
conseguem oferecer escalabilidade horizontal. Isso é, para conseguir mais
recursos de armazenamento e computação para os mainframes você precisa

Nesse contexto, demos início ao estudo do Hadoop, uma tecnologia desenvolvida
utilizando princípios de sistemas distribuídos e que, portanto, se encaixam
nessa nova realidade dos clusters. Passamos por uma descrição alto nível do que
acontece por debaixo dos panos quando nós, desenvolvedores, interagimos com o
HDFS e Yarn.

** Hadoop
Essas duas tecnologias são as peças fundamentais que sustentam o ecossistema
Hadoop. Os nomes HDFS e Yarn podem parecer uma sopa de letrinhas sem muito
sentido para quem os vê fora de contexto. Mas note que ambos na verdade são
siglas bastante descritivas do que essas tecnologias se propõem a fazer.

O HDFS, ou Hadoop Distributed File System, fica encarregado de distribuir e
gerenciar os seus arquivos entre as máquinas do seu cluster. Já o Yarn, Yet
Another Resource Negotiator, é responsável por negociar recrusos computacionais
para as suas tarefas de manipulação de dados.

#+html: <figure align="center">
#+html: <img src="figuras/hadoop_eco.png" />
#+html: <figcaption>Ecossistema Hadoop. Créditos: <a href="https://dzone.com/articles/example-of-etl-application-using-apache-spark-and">dzone</a> </figcaption>
#+html: </figure>

* Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop
** Zookeepr
Serviço de coordenação distribuído.
Fornece as rotas necessárias para as peças do cluster. Identifica nós por nomes (DSN-like).
Ajuda as peças do ecossitemas Hadoop se achar na bagunça toda.
Pode ajudar a evitar concorrência (write on read).
Ajuda na recuperação de falhas.

Para o desenvolvedor, o Zookeeper é completamente transparente.
É mais aparente para quem vai trabalhar com infra.

** Sqoop
Movimenta dados entre banco de dados relacional e HDFS.  Realiza a leitura linha
a linha de tabelas para arquivos (pode pecar na performance).  Permite importar
dados e metadados de bancos de dado relacionais direto para o Hive.  Utiliza
MapReduce por debaixo dos panos: processamento paralelo e tolerante a falha.
Contudo MapReduce já não é mais o motor mais rápido disponível (alternativas:
Spark, Flink).

#+BEGIN_SRC bash
sqoop import \
    --connect jdbc: \
    --username abc \
    --password xyz \
    --table some_table \
    --where "column='something'"
#+END_SRC

*** Desafio
**** SQL
1. Todos os Pokémon lendários
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE legendary=TRUE;
   #+END_SRC
2. Todos os Pokémon de apenas um tipo
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE type2='';
   #+END_SRC
3. Os top 10 Pokémon mais rápidos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY speed DESC
    LIMIT 10;
   #+END_SRC
4. Os top 50 Pokémon com menos HP
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY hp ASC
    LIMIT 50;
   #+END_SRC
5. Os top 100 Pokémon com maiores atributos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY
    greatest(hp, attack, defense, spatk, spdef, speed)
    DESC
    LIMIT 100;
   #+END_SRC

**** Sqoop

** Recursos
http://dontpad.com/aceleracaoeveris
https://drive.google.com/drive/folders/1xaft6H3R3_UvA6-BFHuCvHuWczf6xwqG?usp=sharing

* Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala
Hive e Impala são frontends que possibilitam um a conexão facilitada aos dados no HDFS.

** Hive
Hive oferece a linguagem HQL (Hive Query Langue), uma abstração de alto nível ao MapReduce, com linguagem similar ao conhecido SQL.

Três possíveis engines MapReduce, Spark, Tez.
** Impala
Engine MPP (Massive Parallel Processing). Também uma linguagem similar a SQL (Impala SQL).

Impala não salva os resultados intermediários em disco. Isso acelera e muito o
processamento em comparação com o Hive.

** Diferenças
Casos de uso:
Realtime -> Impala
Batch -> Hive

Impala não salva intermediários em disco, fica tudo em memória. Portanto seu
consumo de memória é muito maior, o que pode ser um limitante dependendo do tipo
de queries executadas.

** Detalhes HQL
CREATE EXTERNAL vs MANAGED TABLE
External - Quando a tablea é apagada, os dados permanecem
Managed - Apaga os dados quando a tabela é deletada

** Formatos de arquivos
*** Parquet
Formato Colunar
*** ORC
Formato Colunar
*** Avro
Formato de Linhas

** Particionamento
Determina como os dados são armazenados.

Pouco particionamento: não faz bom uso da capacidade de paralelismo dos dados
Muito particionamento: pode sobrecarregar o namenode, impactando na performance

** Prática
*** Hive
Utilitários
#+BEGIN_SRC sql
set hive.cli.print.header=true;
set hive.cli.print.current.db=true;
#+END_SRC

Criar tabela em cima de pasta do HDFS.
#+BEGIN_SRC sql
CREATE EXTERNAL TABLE TB_EXT_EMPLOYEE(
id STRING,
groups STRING,
age STRING,
active_lifestyle STRING,
salary STRING)
ROW FORMAT DELIMITED FIELDS
TERMINATED BY '\;'
STORED AS TEXTFILE
LOCATION '/user/hive/warehouse/external/tabelas/employee'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Enviar dados para a LOCATION especificada pela tabela acima.
#+BEGIN_SRC sql
hdfs dfs -put /home/everis/employee.txt /user/hive/warehouse/external/tabelas/employee
#+END_SRC

Melhorar tabela acima com os tipos apropriados.
#+BEGIN_SRC sql
CREATE TABLE TB_EMPLOYEE(
id INT,
groups STRING,
age INT,
active_lifestyle STRING,
salary DOUBLE)
PARTITIONED BY (dt_processamento STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS PARQUET TBLPROPERTIES ("parquet.compression"="SNAPPY");

insert into table TB_EMPLOYEE partition (dt_processamento='20201118') 
select
id,
groups,
age,
active_lifestyle,
salary
from TB_EXT_EMPLOYEE;
#+END_SRC

Criar tabela em cima de pasta no HDFS (segundo exemplo)
#+BEGIN_SRC sql
create external table localidade(
street string,
city string,
zip string,
state string,
beds string,
baths string,
sq_ft string,
type string,
sale_date string,
price string,
latitude string,
longitude string)
PARTITIONED BY (particao STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ","
STORED AS TEXTFILE
location '/user/hive/warehouse/external/tabelas/localidade'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Alternativamente, criar tabela com base em arquivo.
Hive envia dados para o HDFS automaticamente.
#+BEGIN_SRC sql
load data local inpath '/home/everis/base_localidade.csv' 
into table teste.localidade partition (particao='2021-01-21');
#+END_SRC

**** Join
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins

** Dia-a-dia
/home/cloudera/hive/script.sh
#+BEGIN_SRC shell
#!/bin/bash

dt_processamento=$(date '+%Y-%m-%d')
path_file='/home/cloudera/hive/datasets/employee.txt'
table=beca.ext_p_employee
load=/home/cloudera/hive/load.hql

hive -hiveconf dt_processamento=${dt_processamento} -hiveconf table=${table} -hiveconf path_file=${path_file} -f $load 2>> log.txt

hive_status=$?

if [ ${hive_status} -eq 0 ];
then
        echo -e "\nScript executado com sucesso"
else
        echo -e "\nHouve um erro na ingestao do arquivo "

impala-shell -q 'INVALIDATE METADATA beca.ext_p_employee;'

fi
#+END_SRC

/home/cloudera/hive/load.hql
#+BEGIN_SRC shell
LOAD DATA LOCAL INPATH '${hiveconf:path_file}' INTO TABLE ${hiveconf:table} PARTITION(dt_processamento='${hiveconf:dt_processamento}');
#+END_SRC

** Recursos
https://gitlab.com/vmb1/hive

* Explorando o poder do NoSQL com Cassandra e Hbase
Por quê NoSQL em Big Data? Resposta: Performance.

MapReduce processa dados em batch e os dados são acessados de forma sequencial. Ou seja, é preciso percorrer todo o dataset (scan search), mesmo para jobs mais simples.

NoSQL possibilita acesso aleatório aos dados em termos de posição do registro e tempo.

** CAP
Teorema CAP: Consistência, Disponibilidade, Partição Tolerante a Falhas.

** HBase
Banco de dados distribuído e orientado a coluna (Column Family ou Wide Column).

É um Map:
- Esparso
- Distribuído
- Persistente
- Multidimensional
- Ordenado

Depende do Zookeeper para que consiga funcionar. O Zookeeper dá ao HBase visibilidade a todos os nós do cluster.

**** Desvantagem
- Infelizmente não possui uma linguagem de busca (query)
- Não suporta índices em colunas fora da rowkey
- Não suporta tabelas secundárias de índices

**** Vantagem
- Fácil integração ao ecossistema Hadoop

*** Estrutura
- Map indexado por uma linha chave (row key), coluna chave (column key) e uma coluna timestamp.
- Cada valor no Map é interpretado como um vetor de bytes (array of bytes)
- Não distingue tipos (int, str, etc): pode armazenar qualquer tipo de dado, inclusive documentos (JSON, CSV, ...)

{

}

*** Arquitetura
*** Exemplos
Criação de tabela
#+BEGIN_SRC bash

#+END_SRC

Inserção de dados
#+BEGIN_SRC bash

#+END_SRC

Versionamento
#+BEGIN_SRC bash

#+END_SRC

Deleção
#+BEGIN_SRC bash

#+END_SRC

Deleção em coluna versionada
#+BEGIN_SRC bash

#+END_SRC

TTL
#+BEGIN_SRC bash
create 'ttl_exemplo', {'NAME'=>'cf', 'TTL'=>20}
put 'ttl_exemplo', '1', 'cf:nome', 'Informacao'
scan 'ttl_exemplo'
#+END_SRC

** Cassandra
Banco de dados distribuído e orientado a coluna (Column Family ou Wide Column).

Os dados aqui são tipados.

Possui linguagem CQL (SQL-like), porém com algumas operações não suportadas/recomendadas (eg joins, alguns tipos de agrupamento e filto).

Suporta tabela secundárias de índices e filtros em colunas fora da primary key.

*** Arquitetura
Não possui ponto de falha central.

Conexão entre nós é realizada de ponta a ponta, utilizando o protocolo Gossip para distribuição dos dados.

Commit table -> memtable -> SSTable

** TTL
Registro temporário: colunas com propriedade TTL - Time To Live. Os registros são apagados depois desse período.

** Cenários de uso
- Arquitetura baseada em eventos
    Kafka Cluster -> Spark Streaming -> HBase/Cassandra -> Enriquecimento -> Destino

** Recursos
https://github.com/pentguard/DIO-Aceleracao-4-HBase-Cassandra

https://www.datastax.com/

** Etc
#+BEGIN_SRC bash
sudo -u hdfs hadoop dfsadmin -safemode leave
#+END_SRC

Por que o safemode é ativado? Arquivos corrompidos.

É importante parar todos os serviços do Hadoop antes de desligar a máquina.

Como resolver: recuperar os arquivos (caso haja replicação) ou apagar.

* Intensivo de Python: O mínimo que você precisa saber
* Trabalhando com serviços de mensageria real time com Python e Kafka na prática
** Introdução a Micro Serviços
Existem diferentes tipos de arquiteturas.
*** Monolito
Ex: Banco + Aplicação (front + back) tudo no mesmo servidor.
**** Desvantagens
- Forte acoplamento entre diferentes módulos e responsabilidades
**** Vantagens
- Comunicação simplificada entre componentes do sistema
*** Micro Serviços
O sistema é dividido em diversos "módulos" (diversas partes/serviços).
**** Desvantagens
- Complexidade em coordenar a comunicação de diferentes servidores/serviços
- Comunicação pode gerar acoplamento entre os módulos
  - Contudo, esse ponto pode ser solucionado através de um serviço de mensageria!
**** Vantagens
- Isolamento de responsabilidades
  - Melhor manutenibilidade/sustentação
- Controle granulirizado da escala de cada um dos serviços
  - Escalabilidade facilitada

*** Conclusão
[[http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html][Não existe almoço grátis.]]

Talvez um ponto controverso, mas que vale a pena ser considerado: [[https://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html][micro serviços não são um tipo de arquitetura.]]

** Kafka
Sistema de mensageria Open Source.

#+BEGIN_QUOTE
Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.
#+END_QUOTE

#+html: <p align="center"> <figure>
#+html: <img src="figuras/kafka_ex.png" />
#+html: <figcaption>Visão geral dos componentes Kafka. Créditos: <a href="https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a">kavimaluskam@medium</a> </figcaption>
#+html: </figure> </p>

*** Producers
Geram mensagens, que são enviadas para uma fila (mais especificamente tópicos, no Kafka), a serem lidas por Consumers.

#+BEGIN_SRC python
topic = 'topic-name'

# Consumer configuration
# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
conf = {
    'bootstrap.servers': servers,
    'group.id': group_id,
    'session.timeout.ms': 6000,
    'default.topic.config': {'auto.offset.reset': 'smallest'},
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'SCRAM-SHA-256',
    'sasl.username': username,
    'sasl.password': password,
}
p = Producer(conf)

try:
    p.produce(topic, "my message new 2", callback=delivery_callback)
except BufferError as e:
    print('%% Local producer queue is full (%d messages awaiting delivery): try again\n',
          len(p))
p.poll(0)

print('%% Waiting for %d deliveries\n' % len(p))
p.flush()
#+END_SRC

*** Consumers
Subscrevem a determinadas filas (tópicos) e continuamente consomem as mensagens geradas pelo Producer

#+BEGIN_SRC python
topics = ['topic-name']

# Consumer configuration
# See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
conf = {
    'bootstrap.servers': servers,
    'group.id': group_id,
    'session.timeout.ms': 6000,
    'default.topic.config': {'auto.offset.reset': 'smallest'},
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'SCRAM-SHA-256',
    'sasl.username': username,
    'sasl.password': password,
}

c = Consumer(conf)
c.subscribe(topics)
try:
    while True:
        msg = c.poll(timeout=1.0)
        if msg is None:
            continue
        if msg.error():
            # Error or event
            if msg.error().code() == KafkaError._PARTITION_EOF:
                # End of partition event
                sys.stderr.write('%% %s [%d] reached end at offset %d\n' %
                                 (msg.topic(), msg.partition(), msg.offset()))
            elif msg.error():
                # Error
                raise KafkaException(msg.error())
        else:
            # Proper message
            sys.stderr.write('%% %s [%d] at offset %d with key %s:\n' %
                             (msg.topic(), msg.partition(), msg.offset(),
                              str(msg.key())))
            print(msg.value())

except KeyboardInterrupt:
    sys.stderr.write('%% Aborted by user\n')

# Close down consumer to commit final offsets.
c.close()
#+END_SRC

** Kafka as a Service
Karafka - Managed Apache Kafka Cluster

https://www.cloudkarafka.com/

5 tópicos grátis (free-tier)

** Recursos
https://github.com/huguinho-alves/python_aceleracao_everys

[[https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a][Start your real-time pipeline with Apache Kafka]]

[[https://stackoverflow.com/questions/4127241/orchestration-vs-choreography#:~:text=The%20choreography%20describes%20the%20interactions,the%20services%20involved%20should%20reside.][Orchestration vs. Choreography]]

[[http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html][Microservices - Not A Free Lunch!]]

[[https://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html][Clean Micro-service Architecture]]

* Processando grandes conjuntos de dados de forma paralela e distribuída com Spark
#+BEGIN_QUOTE
Apache Spark is a unified analytics engine for large-scale data processing.
#+END_QUOTE

** Databricks
Distribuição comercial do Apache Spark.

** Diferencial do Spark
Executa processamento de dados até 100 vezes mais rápido que o MapReduce tradicional.

#+BEGIN_QUOTE
They used Spark and sorted 100TB of data using 206 EC2 i2.8xlarge machines in 23 minutes. The previous world record was 72 minutes, set by a Hadoop MapReduce cluster of 2100 nodes.
#+END_QUOTE

https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html

*** O truque
Velocidade de acesso:

Cache > RAM > Disco

#+html: <p align="center"> <figure>
#+html: <img src="figuras/memory_speed.png" />

#+html: <figcaption>Tipos de memória. Créditos: <a href="https://medium.com/@esmerycornielle/the-cpu-and-the-memory-2eb300d6c72d">esmerycornielle@medium</a></figcaption>
#+html: </figure> </p>

Spark, diferentemente do MapReduce, traz os dados para RAM. Isso diminui consideravelmente o tempo de transformação dos dados.

Trata-se, portanto, de um framework in-memory.

*** Linguagens suportadas
- Scala
- Java
- Python
- R
- SQL

** Arquitetura
#+html: <p align="center"> <figure>
#+html: <img src="figuras/spark_overview.png" />
#+html: <figcaption>Arquitetura do Apache Spark. Créditos: <a href="https://spark.apache.org/docs/2.0.0/cluster-overview.html">Documentação</a></figcaption>
#+html: </figure> </p>

Driver Node contém o Spark Context, responsável por gerir os recursos a serem utilizados no processamento dos dados

- Worker Node = um nó (máquina) do cluster

- Cada Worker Node pode conter um ou mais Executors

- Cada Executor processa uma ou mais Tasks

- Executors avisam ao Spark Context o progresso de suas Tasks

  - Caso um Executor deixe de responder (máquina caiu) o Spark Context consegue criar novos Executores em outro Worker Node para resumir o processamento interrompido

*** Divisão
Apache Spark pode ser quebrado em cinco bibliotecas:

- SparkSQL
  Processamento de dados tabulares
- Spark Streaming
  Micro-batch de dados
- MLlib
  Machine Learning
- GraphX
  Dados em grafos

Todos são sustentados pelo Spark Core

** RDD
Resilient Distributed Dataset é a principal abstração do Spark.

- Resilient: Dado pode ser recuperado facilmente em caso de falhas

- Distributed: Pode ser processado por diferentes máquinas

RDDs são imutáveis. Transformações sobre RDDs geram novas RDDs.
** Exemplos
- Context
  #+BEGIN_SRC scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName("Simple Application")
val sc = new SparkContext(conf)
  #+END_SRC

- Leitura de dados
  #+BEGIN_SRC scala
val df = spark.read.format("csv").option("sep",",").option("header","true").load("file:///home/everis/avengers.csv")

insurance.show(10, false)

val df_url = insurance.select("URL")
  #+END_SRC

- Manipulação
  #+BEGIN_SRC scala
df.select("field1","field2").show()
df.select($"field1", $"field2"+1).show()
df.groupBy("age").count().show()
  #+END_SRC

- SQL
  #+BEGIN_SRC scala
df.createOrReplaceTempView("av")

spark.sql("SELECT Appearances FROM av where URL LIKE '%Iron_Man%'").show()
  #+END_SRC

  - UDF
    User Defined Function
    #+BEGIN_SRC scala
val squared = (s: Long) => {
  s * s
}
spark.udf.register("square", squared)

spark.sql("SELECT square(Appearances) FROM av").show()
    #+END_SRC

** Recursos
[[https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html][Spark wins Daytona Gray Sort 100TB Benchmark]]

[[http://spark.apache.org/docs/2.4.0/cluster-overview.html][Cluster Mode Overview]]

[[https://docs.microsoft.com/pt-br/azure/databricks/spark/latest/spark-sql/udf-scala][Funções definidas pelo usuário – Scala]]

[[https://medium.com/@esmerycornielle/the-cpu-and-the-memory-2eb300d6c72d][The CPU and The Memory]]

[[https://www.udacity.com/course/learn-spark-at-udacity--ud2002][Learn Spark at Udacity (Free)]]

[[https://github.com/databricks/koalas][Koalas (Pandas from Spark)]]

* Próximas aulas
** Criando pipelines de dados eficientes - Parte 1
** Criando pipelines de dados eficientes - Parte 2
** Orquestrando Big Data em Ambiente de Nuvem
** Scala: o poder de uma linguagem multiparadigma
** O que você precisa saber para construir APIs verdadeiramente restfull

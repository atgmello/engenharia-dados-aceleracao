:PROPERTIES:
:TOC:      :include all :depth 2
:END:
#+TITLE: Engenharia de Dados
:CONTENTS:
[[#abertura---como-trabalhar-em-um-ambiente-%C3%A1gil-e-global][Abertura - Como trabalhar em um ambiente ágil e global]]
[[#monitoramento-de-clusters-hadoop-de-alto-n%C3%ADvel-com-hdfs-e-yarn][Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn]]
[[#orquestrando-ambientes-de-big-data-distruibuidos-com-zookeeper-%2C-yarn-e-sqoop][Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop]]
[[#como-realizar-consultas-de-maneira-simples-no-ambiente-complexo-de-big-data-com-hive-e-impala][Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala]]
:END:

* Abertura - Como trabalhar em um ambiente ágil e global
A live começou com a apresentação da Everis, empresa de consultoria
multinacional que estará ministrando as palestras técnicas sobre o assunto de
Big Data. Entraram em detalhes da filosofia e cultura da empresa, bem como
ressaltaram que há oportunidades na área de Dados e que poderão ser oferecidas
aos alunos de destaque do curso ao fim do treinamento.

Ressaltaram que tiveram um total de 2000 candidaturas. Por volta de 400 foram
selecionados após realizar uma prova eliminatória. Prova contou com questões de
multipla escolha sobre Python e lógica de programação, além de um coding test.

A seguir trataram um pouco sobre principais tecnologias que serão cobertas ao
longo do curso. Serão 14 dias de treinamento passando pelas linguagens de
programação Python e Scala. Também serão dadas noções do core Hadoop com HDFS,
Yarn, além de outras ferramentas do ecossistema como MapReduce, Sqoop, Hive,
Zookeeper e Spark. Também serão cobertas noções de bancos de dados NoSQL como
Cassandra e HBase.

Ao longo da apresentação também focaram na importância da mudança do paradigma
Waterfall para o paradigma Agile para projetos de Engenharia de Dados.
Planejamento e execução de Sprints, Dailies, Reviews, organização em Squads
autosuficientes, constante interação com o cliente, foco na entrega. São algumas
das características essenciais da cultura Agile. Agile requer um processo de
comunicação forte, com transparência, confiânça e união entre os colaboradores
das Squads. No geral, focaram no fato de que colaboração é um aspecto
fundamental do framework Agile.

* Monitoramento de clusters Hadoop de alto nível com HDFS e Yarn
** Sistemas distribuídos
Clusters de computadores hoje em dia são uma alternativa aos antigos mainframes.
Enquanto mainframes conseguem oferecer escalabilidade vertical, cluters
conseguem oferecer escalabilidade horizontal. Isso é, para conseguir mais
recursos de armazenamento e computação para os mainframes você precisa

Nesse contexto, demos início ao estudo do Hadoop, uma tecnologia desenvolvida
utilizando princípios de sistemas distribuídos e que, portanto, se encaixam
nessa nova realidade dos clusters. Passamos por uma descrição alto nível do que
acontece por debaixo dos panos quando nós, desenvolvedores, interagimos com o
HDFS e Yarn.

** Hadoop
Essas duas tecnologias são as peças fundamentais que sustentam o ecossistema
Hadoop. Os nomes HDFS e Yarn podem parecer uma sopa de letrinhas sem muito
sentido para quem os vê fora de contexto. Mas note que ambos na verdade são
siglas bastante descritivas do que essas tecnologias se propõem a fazer.

O HDFS, ou Hadoop Distributed File System, fica encarregado de distribuir e
gerenciar os seus arquivos entre as máquinas do seu cluster. Já o Yarn, Yet
Another Resource Negotiator, é responsável por negociar recrusos computacionais
para as suas tarefas de manipulação de dados.

[[file:./figuras/hadoop_eco.png]]
Créditos: https://dzone.com/articles/example-of-etl-application-using-apache-spark-and

* Orquestrando ambientes de big data distruibuidos com Zookeeper, Yarn e Sqoop
** Zookeepr
Serviço de coordenação distribuído.
Fornece as rotas necessárias para as peças do cluster. Identifica nós por nomes (DSN-like).
Ajuda as peças do ecossitemas Hadoop se achar na bagunça toda.
Pode ajudar a evitar concorrência (write on read).
Ajuda na recuperação de falhas.

Para o desenvolvedor, o Zookeeper é completamente transparente.
É mais aparente para quem vai trabalhar com infra.

** Sqoop
Movimenta dados entre banco de dados relacional e HDFS.  Realiza a leitura linha
a linha de tabelas para arquivos (pode pecar na performance).  Permite importar
dados e metadados de bancos de dado relacionais direto para o Hive.  Utiliza
MapReduce por debaixo dos panos: processamento paralelo e tolerante a falha.
Contudo MapReduce já não é mais o motor mais rápido disponível (alternativas:
Spark, Flink).

#+BEGIN_SRC bash
sqoop import \
    --connect jdbc: \
    --username abc \
    --password xyz \
    --table some_table \
    --where "column='something'"
#+END_SRC

*** Desafio
**** SQL
1. Todos os Pokémon lendários
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE legendary=TRUE;
   #+END_SRC
2. Todos os Pokémon de apenas um tipo
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    WHERE type2='';
   #+END_SRC
3. Os top 10 Pokémon mais rápidos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY speed DESC
    LIMIT 10;
   #+END_SRC
4. Os top 50 Pokémon com menos HP
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY hp ASC
    LIMIT 50;
   #+END_SRC
5. Os top 100 Pokémon com maiores atributos
   #+BEGIN_SRC sql
    SELECT *
    FROM trainning.pokemon
    ORDER BY
    greatest(hp, attack, defense, spatk, spdef, speed)
    DESC
    LIMIT 100;
   #+END_SRC

**** Sqoop

** Recursos
http://dontpad.com/aceleracaoeveris
https://drive.google.com/drive/folders/1xaft6H3R3_UvA6-BFHuCvHuWczf6xwqG?usp=sharing

* Como realizar consultas de maneira simples no ambiente complexo de Big Data com HIVE e Impala
Hive e Impala são frontends que possibilitam uma conexão facilitada aos dados no HDFS.

** Hive
Hive oferece a linguagem HQL (Hive Query Langue), uma abstração de alto nível ao MapReduce, com linguagem similar ao conhecido SQL.

Três possíveis engines MapReduce, Spark, Tez.
** Impala
Engine MPP (Massive Parallel Processing). Também uma linguagem similar a SQL (Impala SQL).

Impala não salva os resultados intermediários em disco. Isso acelera e muito o
processamento em comparação com o Hive.

** Diferenças
Casos de uso:
Realtime -> Impala
Batch -> Hive

Impala não salva intermediários em disco, fica tudo em memória. Portanto seu
consumo de memória é muito maior, o que pode ser um limitante dependendo do tipo
de queries executadas.

** Detalhes HQL
CREATE EXTERNAL vs MANAGED TABLE
External - Quando a tablea é apagada, os dados permanecem
Managed - Apaga os dados quando a tabela é deletada

** Formatos de arquivos
*** Parquet
Formato Colunar
*** ORC
Formato Colunar
*** Avro
Formato de Linhas

** Particionamento
Determina como os dados são armazenados.

Pouco particionamento: não faz bom uso da capacidade de paralelismo dos dados
Muito particionamento: pode sobrecarregar o namenode, impactando na performance

** Prática
*** Hive
Utilitários
#+BEGIN_SRC sql
set hive.cli.print.header=true;
set hive.cli.print.current.db=true;
#+END_SRC

Criar tabela em cima de pasta do HDFS.
#+BEGIN_SRC sql
CREATE EXTERNAL TABLE TB_EXT_EMPLOYEE(
id STRING,
groups STRING,
age STRING,
active_lifestyle STRING,
salary STRING)
ROW FORMAT DELIMITED FIELDS
TERMINATED BY '\;'
STORED AS TEXTFILE
LOCATION '/user/hive/warehouse/external/tabelas/employee'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Enviar dados para a LOCATION especificada pela tabela acima.
#+BEGIN_SRC sql
hdfs dfs -put /home/everis/employee.txt /user/hive/warehouse/external/tabelas/employee
#+END_SRC

Melhorar tabela acima com os tipos apropriados.
#+BEGIN_SRC sql
CREATE TABLE TB_EMPLOYEE(
id INT,
groups STRING,
age INT,
active_lifestyle STRING,
salary DOUBLE)
PARTITIONED BY (dt_processamento STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS PARQUET TBLPROPERTIES ("parquet.compression"="SNAPPY");

insert into table TB_EMPLOYEE partition (dt_processamento='20201118') 
select
id,
groups,
age,
active_lifestyle,
salary
from TB_EXT_EMPLOYEE;
#+END_SRC

Criar tabela em cima de pasta no HDFS (segundo exemplo)
#+BEGIN_SRC sql
create external table localidade(
street string,
city string,
zip string,
state string,
beds string,
baths string,
sq_ft string,
type string,
sale_date string,
price string,
latitude string,
longitude string)
PARTITIONED BY (particao STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ","
STORED AS TEXTFILE
location '/user/hive/warehouse/external/tabelas/localidade'
tblproperties ("skip.header.line.count"="1");
#+END_SRC

Alternativamente, criar tabela com base em arquivo.
Hive envia dados para o HDFS automaticamente.
#+BEGIN_SRC sql
load data local inpath '/home/everis/base_localidade.csv' 
into table teste.localidade partition (particao='2021-01-21');
#+END_SRC

**** Join
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins

** Dia-a-dia
/home/cloudera/hive/script.sh
#+BEGIN_SRC shell
#!/bin/bash

dt_processamento=$(date '+%Y-%m-%d')
path_file='/home/cloudera/hive/datasets/employee.txt'
table=beca.ext_p_employee
load=/home/cloudera/hive/load.hql

hive -hiveconf dt_processamento=${dt_processamento} -hiveconf table=${table} -hiveconf path_file=${path_file} -f $load 2>> log.txt

hive_status=$?

if [ ${hive_status} -eq 0 ];
then
        echo -e "\nScript executado com sucesso"
else
        echo -e "\nHouve um erro na ingestao do arquivo "

impala-shell -q 'INVALIDATE METADATA beca.ext_p_employee;'

fi
#+END_SRC

/home/cloudera/hive/load.hql
#+BEGIN_SRC shell
LOAD DATA LOCAL INPATH '${hiveconf:path_file}' INTO TABLE ${hiveconf:table} PARTITION(dt_processamento='${hiveconf:dt_processamento}');
#+END_SRC

** Recursos
https://gitlab.com/vmb1/hive

* Explorando o poder do NoSQL com Cassandra e Hbase
* Intensivo de Python: O mínimo que você precisa saber
* Trabalhando com serviços de mensageria real time com Python e Kafka na prática
* Processando grandes conjuntos de dados de forma paralela e distribuída com Spark
* Criando pipelines de dados eficientes - Parte 1
* Criando pipelines de dados eficientes - Parte 2
* Orquestrando Big Data em Ambiente de Nuvem
* Scala: o poder de uma linguagem multiparadigma
* O que você precisa saber para construir APIs verdadeiramente restfull
